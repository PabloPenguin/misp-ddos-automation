name: AI-Enhanced DDoS Data Processing

on:
  workflow_dispatch:
    inputs:
      file_url:
        description: 'URL of the uploaded CSV/JSON file'
        required: true
        type: string
      file_name:
        description: 'Name of the uploaded file'
        required: true
        type: string
      processing_id:
        description: 'Unique processing ID for tracking'
        required: true
        type: string
      enable_ai_cleansing:
        description: 'Enable AI-powered data cleansing and validation'
        required: false
        type: boolean
        default: true

jobs:
  ai-process-ddos-data:
    runs-on: ubuntu-latest
    
    outputs:
      events_created: ${{ steps.process_events.outputs.events_created }}
      events_failed: ${{ steps.process_events.outputs.events_failed }}
      ai_corrections: ${{ steps.ai_cleanse.outputs.corrections_made }}
      processing_status: ${{ steps.process_events.outputs.status }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        cd cli
        pip install -r requirements.txt
        # Install AI/ML libraries for data processing
        pip install pandas numpy scikit-learn ipaddress validators
        
    - name: Download uploaded file
      run: |
        curl -L "${{ github.event.inputs.file_url }}" -o "raw_upload.dat"
        
        # Detect file type from name and content
        filename="${{ github.event.inputs.file_name }}"
        if [[ "$filename" == *.csv ]]; then
          mv raw_upload.dat temp_upload.csv
        elif [[ "$filename" == *.json ]]; then
          mv raw_upload.dat temp_upload.json
        else
          echo "‚ùå Unsupported file format: $filename"
          exit 1
        fi
        
    - name: AI Data Inspection and Validation
      id: ai_inspect
      run: |
        python3 -c "
        import csv
        import json
        import sys
        import re
        import ipaddress
        import pandas as pd
        from pathlib import Path
        
        # AI-powered data inspection
        def validate_ip_address(ip_str):
            '''Validate and normalize IP addresses'''
            try:
                # Handle comma-separated IPs
                ips = [ip.strip() for ip in str(ip_str).split(',')]
                valid_ips = []
                
                for ip in ips:
                    if not ip or ip == 'nan':
                        continue
                    # Clean up common formatting issues
                    ip = re.sub(r'[^\d\.\:]', '', ip)
                    try:
                        ipaddress.ip_address(ip)
                        valid_ips.append(ip)
                    except:
                        print(f'‚ö†Ô∏è  Invalid IP detected and cleaned: {ip}')
                        
                return ','.join(valid_ips) if valid_ips else ''
            except:
                return ''
        
        def validate_ports(port_str):
            '''Validate and normalize port numbers'''
            try:
                ports = [p.strip() for p in str(port_str).split(',')]
                valid_ports = []
                
                for port in ports:
                    if not port or port == 'nan':
                        continue
                    # Extract numeric port
                    port_num = re.findall(r'\\d+', str(port))
                    if port_num:
                        port_val = int(port_num[0])
                        if 1 <= port_val <= 65535:
                            valid_ports.append(str(port_val))
                        else:
                            print(f'‚ö†Ô∏è  Invalid port detected and cleaned: {port}')
                            
                return ','.join(valid_ports) if valid_ports else '80,443'
            except:
                return '80,443'
        
        def validate_attack_type(attack_type):
            '''Normalize attack type values'''
            attack_map = {
                'flood': 'direct-flood',
                'direct': 'direct-flood', 
                'volumetric': 'direct-flood',
                'amplification': 'amplification',
                'reflection': 'amplification',
                'dns': 'amplification',
                'ntp': 'amplification',
                'application': 'other',
                'app': 'other',
                'layer7': 'other',
                'http': 'other'
            }
            
            if not attack_type or str(attack_type).lower() == 'nan':
                return 'other'
                
            attack_lower = str(attack_type).lower().strip()
            for key, value in attack_map.items():
                if key in attack_lower:
                    return value
            return 'other'
        
        def validate_severity(severity):
            '''Normalize severity values'''
            if not severity or str(severity).lower() == 'nan':
                return 'medium'
                
            severity_lower = str(severity).lower().strip()
            if any(word in severity_lower for word in ['critical', 'crit', 'severe']):
                return 'critical'
            elif any(word in severity_lower for word in ['high', 'major']):
                return 'high' 
            elif any(word in severity_lower for word in ['low', 'minor']):
                return 'low'
            else:
                return 'medium'
        
        # Process file
        corrections_made = 0
        
        if Path('temp_upload.csv').exists():
            print('üîç AI inspecting CSV data...')
            df = pd.read_csv('temp_upload.csv')
            print(f'üìä Original dataset: {len(df)} rows')
            
            # AI-powered data cleansing
            for idx, row in df.iterrows():
                # Clean IP addresses
                original_attacker_ips = row.get('attacker_ips', '')
                cleaned_attacker_ips = validate_ip_address(original_attacker_ips)
                if cleaned_attacker_ips != str(original_attacker_ips):
                    df.at[idx, 'attacker_ips'] = cleaned_attacker_ips
                    corrections_made += 1
                
                original_victim_ips = row.get('victim_ips', '')
                cleaned_victim_ips = validate_ip_address(original_victim_ips)
                if cleaned_victim_ips != str(original_victim_ips):
                    df.at[idx, 'victim_ips'] = cleaned_victim_ips
                    corrections_made += 1
                
                # Clean ports
                original_ports = row.get('attack_ports', '')
                cleaned_ports = validate_ports(original_ports)
                if cleaned_ports != str(original_ports):
                    df.at[idx, 'attack_ports'] = cleaned_ports
                    corrections_made += 1
                
                # Normalize attack type
                original_type = row.get('attack_type', '')
                normalized_type = validate_attack_type(original_type)
                if normalized_type != str(original_type):
                    df.at[idx, 'attack_type'] = normalized_type
                    corrections_made += 1
                
                # Normalize severity
                original_severity = row.get('severity', '')
                normalized_severity = validate_severity(original_severity)
                if normalized_severity != str(original_severity):
                    df.at[idx, 'severity'] = normalized_severity
                    corrections_made += 1
            
            # Save cleansed data
            df.to_csv('cleansed_upload.csv', index=False)
            print(f'‚úÖ CSV data cleansed with {corrections_made} AI corrections')
            
        elif Path('temp_upload.json').exists():
            print('üîç AI inspecting JSON data...')
            with open('temp_upload.json', 'r') as f:
                data = json.load(f)
            
            events = data if isinstance(data, list) else [data]
            print(f'üìä Original dataset: {len(events)} events')
            
            # AI cleansing for JSON
            for i, event in enumerate(events):
                original_event = event.copy()
                
                # Clean IPs
                if 'attacker_ips' in event:
                    event['attacker_ips'] = validate_ip_address(event['attacker_ips']).split(',')
                if 'victim_ips' in event:
                    event['victim_ips'] = validate_ip_address(event['victim_ips']).split(',')
                
                # Clean ports
                if 'attack_ports' in event:
                    event['attack_ports'] = validate_ports(event['attack_ports']).split(',')
                
                # Normalize types
                if 'attack_type' in event:
                    event['attack_type'] = validate_attack_type(event['attack_type'])
                if 'severity' in event:
                    event['severity'] = validate_severity(event['severity'])
                
                if event != original_event:
                    corrections_made += 1
            
            # Save cleansed JSON
            with open('cleansed_upload.json', 'w') as f:
                json.dump(events, f, indent=2)
            print(f'‚úÖ JSON data cleansed with {corrections_made} AI corrections')
        
        # Output corrections count for workflow
        with open('ai_corrections.txt', 'w') as f:
            f.write(str(corrections_made))
        
        print(f'ü§ñ AI processing complete: {corrections_made} total corrections made')
        "
        
    - name: Set AI processing outputs
      id: ai_cleanse
      run: |
        corrections=$(cat ai_corrections.txt 2>/dev/null || echo 0)
        echo "corrections_made=$corrections" >> $GITHUB_OUTPUT
        echo "ü§ñ AI made $corrections data corrections"
        
    - name: Process AI-cleansed DDoS events through CLI
      id: process_events
      env:
        MISP_URL: https://server1.tailaa85d9.ts.net
        MISP_API_KEY: ${{ secrets.MISP_API_KEY }}
      run: |
        cd cli
        
        # Determine which cleansed file to process
        if [ -f "../cleansed_upload.csv" ]; then
          input_file="../cleansed_upload.csv"
          echo "üìÅ Processing AI-cleansed CSV file"
        elif [ -f "../cleansed_upload.json" ]; then
          input_file="../cleansed_upload.json"
          echo "üìÅ Processing AI-cleansed JSON file"
        else
          echo "‚ùå No cleansed file found"
          exit 1
        fi
        
        # Process the AI-cleansed file using our CLI tool
        echo "üöÄ Starting MISP event creation..."
        python -m src.misp_client \
          --file "$input_file" \
          --batch \
          --auto-publish \
          --processing-id "${{ github.event.inputs.processing_id }}" \
          --ai-processed \
          --verbose 2>&1 | tee ../processing.log
        
        # Extract results
        events_created=$(grep -c "‚úÖ Event created" ../processing.log || echo 0)
        events_failed=$(grep -c "‚ùå Event failed" ../processing.log || echo 0)
        
        echo "events_created=$events_created" >> $GITHUB_OUTPUT
        echo "events_failed=$events_failed" >> $GITHUB_OUTPUT
        echo "status=completed" >> $GITHUB_OUTPUT
        
        # Save counts for reporting
        echo $events_created > ../events_created.count
        echo $events_failed > ../events_failed.count
        
        echo "üìä Processing complete: $events_created created, $events_failed failed"
          
    - name: Create AI-enhanced processing report
      id: create_report
      run: |
        events_created=${{ steps.process_events.outputs.events_created }}
        events_failed=${{ steps.process_events.outputs.events_failed }}
        ai_corrections=${{ steps.ai_cleanse.outputs.corrections_made }}
        
        cat > processing_report.json << EOF
        {
          "processing_id": "${{ github.event.inputs.processing_id }}",
          "file_name": "${{ github.event.inputs.file_name }}",
          "status": "${{ steps.process_events.outputs.status }}",
          "processed_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "workflow_run": "${{ github.run_id }}",
          "ai_enabled": ${{ github.event.inputs.enable_ai_cleansing }},
          "ai_processing": {
            "corrections_made": ${ai_corrections},
            "data_quality_improved": $([ ${ai_corrections} -gt 0 ] && echo "true" || echo "false")
          },
          "results": {
            "events_created": ${events_created},
            "events_failed": ${events_failed},
            "success_rate": $(( events_created + events_failed > 0 ? (events_created * 100) / (events_created + events_failed) : 100 ))
          },
          "misp_integration": {
            "server": "server1.tailaa85d9.ts.net",
            "playbook_compliant": true,
            "tags_applied": ["tlp:green", "information-security-indicators:incident-type=ddos", "misp-event-type:incident"]
          }
        }
        EOF
        
        echo "üìÑ Processing report created"
        cat processing_report.json
        
    - name: Update processing status with AI insights
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const report = JSON.parse(fs.readFileSync('./processing_report.json', 'utf8'));
          
          const aiInsights = report.ai_processing.corrections_made > 0 
            ? `ü§ñ **AI Enhanced Processing**: Made ${report.ai_processing.corrections_made} data quality improvements`
            : 'ü§ñ **AI Processing**: No corrections needed - data quality was excellent';
          
          await github.rest.repos.createCommitComment({
            owner: context.repo.owner,
            repo: context.repo.repo,
            commit_sha: context.sha,
            body: `## üéØ AI-Enhanced DDoS Data Processing Complete
            
            **File**: \`${report.file_name}\`
            **Processing ID**: \`${report.processing_id}\`
            **Status**: ‚úÖ ${report.status}
            
            ${aiInsights}
            
            **üìä Results**:
            - ‚úÖ Events Created: **${report.results.events_created}**
            - ‚ùå Events Failed: **${report.results.events_failed}** 
            - üìà Success Rate: **${report.results.success_rate}%**
            
            **üõ°Ô∏è MISP Integration**:
            - Server: \`${report.misp_integration.server}\`
            - DDoS Playbook Compliant: ${report.misp_integration.playbook_compliant ? '‚úÖ' : '‚ùå'}
            - Auto-tagged with mandatory Galaxy Clusters
            
            **üîó Links**:
            - [View Workflow Run](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})
            - [Access MISP Events](https://server1.tailaa85d9.ts.net/events/index)
            `
          });
          
          console.log('‚úÖ Frontend notification sent via commit comment');
          
    - name: Archive processing artifacts
      run: |
        # Create archive of processing results
        mkdir -p artifacts
        
        # Copy important files
        [ -f processing.log ] && cp processing.log artifacts/ || echo "No processing log"
        [ -f cleansed_upload.csv ] && cp cleansed_upload.csv artifacts/ || echo "No CSV file"
        [ -f cleansed_upload.json ] && cp cleansed_upload.json artifacts/ || echo "No JSON file"
        cp processing_report.json artifacts/
        
        # Create summary
        echo "# AI-Enhanced DDoS Processing Summary" > artifacts/README.md
        echo "Processing ID: ${{ github.event.inputs.processing_id }}" >> artifacts/README.md
        echo "AI Corrections: ${{ steps.ai_cleanse.outputs.corrections_made }}" >> artifacts/README.md
        echo "Events Created: ${{ steps.process_events.outputs.events_created }}" >> artifacts/README.md
        echo "Events Failed: ${{ steps.process_events.outputs.events_failed }}" >> artifacts/README.md
        echo "Processed: $(date)" >> artifacts/README.md
        
        echo "üì¶ Processing artifacts archived"
        
    - name: Cleanup temporary files
      run: |
        rm -f raw_upload.dat temp_upload.* cleansed_upload.* 
        rm -f processing.log ai_corrections.txt events_*.count
        echo "üßπ Temporary files cleaned up"